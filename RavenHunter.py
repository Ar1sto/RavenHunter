import requests
import sqlite3
import asyncio
import argparse
import json
import csv
import os
import sys
import re
import platform
import subprocess
from tqdm import tqdm
from datetime import datetime
from colorama import init as enable
from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError
from bs4 import BeautifulSoup

enable()
if os.name == "nt":
    os.system("chcp 65001 > nul")

DB_NAME = "cia_docs.db"

BASE_URL = "https://www.cia.gov/readingroom/search/site/*?page={}"
DOCUMENT_PREFIX = "https://www.cia.gov/readingroom/document/"

HEADERS = {
    "User-Agent": "Mozilla/5.0 (compatible; Project-RavenHunter/4.0)"
}

banner = """
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣀⣀⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣴⣿⣿⡟⠋⢻⣷⣄⡀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣤⣾⣿⣷⣿⣿⣿⣿⣿⣶⣾⣿⣿⠿⠿⠿⠶⠄⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⠉⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡟⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠃⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠃⠀⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⣿⣿⣿⣿⣿⣿⣿⡟⠁⠀⠀⠀⠀⠀
╔═══════════════════════════════════════════════════╗⠀⠀⠀⠀
║ ╦═╗┌─┐┬  ┬┌─┐┌┐┌╦ ╦┬ ┬┌┐┌┌┬┐┌─┐┬─┐ v4.0.1         ║
║ ╠╦╝├─┤└┐┌┘├┤ │││╠═╣│ ││││ │ ├┤ ├┬┘                ║
║ ╩╚═┴ ┴ └┘ └─┘┘└┘╩ ╩└─┘┘└┘ ┴ └─┘┴└─ ~ by 0mniscius ║
║ [+] CIA RDP-Document Scraping & Dumping Tool      ║
╚═══════════════════════════════════════════════════╝⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠸⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠉⠙⠛⠋⠉⠉⠀⠀⠀⠀⠀⠀
"""

# === DATABASE SETUP ===

def init_db():
    conn = sqlite3.connect(DB_NAME)
    c = conn.cursor()
    c.execute("""CREATE TABLE IF NOT EXISTS found_links (
                    url TEXT PRIMARY KEY,
                    timestamp TEXT
                )""")
    c.execute("""CREATE TABLE IF NOT EXISTS last_page (
                    id INTEGER PRIMARY KEY CHECK (id = 1),
                    page_number INTEGER DEFAULT 0
                )""")
    c.execute('INSERT OR IGNORE INTO last_page (id, page_number) VALUES (1, 0)')
    conn.commit()
    return conn, c

# === LOGIC ===

def get_last_page(conn):
    c = conn.cursor()
    c.execute("SELECT page_number FROM last_page WHERE id = 1")
    row = c.fetchone()
    return row[0] if row else 0

def export_found_links(format="json"):
    links = get_all_found_links()
    if not links:
        print("No links found to export.")
        return

    header = f"Generated by *RavenHunter*\n{banner}\n"

    if format == "json":
        with open("cia_docs_export.json", "w", encoding="utf-8") as f:
            f.write(header)
            json.dump(links, f, indent=4)
        print("[✔] Export as JSON completed: cia_docs_export.json")

    elif format == "csv":
        with open("cia_docs_export.csv", "w", newline='', encoding="utf-8") as f:
            f.write(header)
            writer = csv.writer(f)
            writer.writerow(["URL"])
            for url in links:
                writer.writerow([url])
        print("[✔] Export as CSV completed: cia_docs_export.csv")

    else:
        print(f"[!] Unknown export format: {format}")

def get_all_found_links():
    with sqlite3.connect(DB_NAME) as conn:
        c = conn.execute("SELECT url FROM found_links")
        return [row[0] for row in c.fetchall()]

def show_summary():
    with sqlite3.connect(DB_NAME) as conn:
        c = conn.execute("SELECT COUNT(*) FROM found_links")
        count = c.fetchone()[0]
        print(f"[✔] Documents already extracted: {count}")

# === SCRAPE FUNCTIONS ===

def transform_link(original_link):
    doc_id = original_link.split("/document/")[-1]
    doc_id_upper = doc_id.upper()
    return f"https://www.cia.gov/readingroom/docs/{doc_id_upper}.pdf"

def extract_total_pages(html):
    match = re.search(r'pager-last.*?page=(\d+)', html)
    return int(match.group(1)) if match else -1

async def get_total_pages_from_page(page):
    try:
        await page.wait_for_selector("li.pager-last a", timeout=5000)
        content = await page.content()
        return extract_total_pages(content)
    except Exception as e:
        print(f"[ERROR] Failed to get total pages: {e}")
        return -1

def tor_service_running():
    try:
        if platform.system() == "Windows":
            result = subprocess.check_output("tasklist", shell=True).decode()
            return "tor.exe" in result.lower()
        else:
            result = subprocess.run(["pgrep", "tor"], stdout=subprocess.DEVNULL)
            return result.returncode == 0
    except Exception:
        return False

async def scrape_documents(timeout, resume_page, verbose, conn, c, use_tor=False):
    page_number = resume_page
    total_pages = -1

    async with async_playwright() as p:
        proxy = {"server": "socks5://127.0.0.1:9050"} if use_tor else None
        browser = await p.chromium.launch(headless=False, proxy=proxy)
        context = await browser.new_context(extra_http_headers=HEADERS)
        page = await context.new_page()

        while True:
            url = BASE_URL.format(page_number)
            print(f"\n[INFO] Loading page {page_number}...")
            try:
                await page.goto(url, wait_until="networkidle", timeout=1000000)
                if total_pages == -1:
                    total_pages = await get_total_pages_from_page(page)
                print(f"[INFO] Loading page {page_number}/{total_pages}...")
                await page.wait_for_selector("h3.title a", timeout=5000)
                print("[INFO] Retrieving HTML content...")
                content = await page.content()

                soup = BeautifulSoup(content, "html.parser")
                h3_titles = soup.find_all('h3', class_='title')
                document_links = [a['href'] for h3 in h3_titles if (a := h3.find('a', href=True)) and a['href'].startswith(DOCUMENT_PREFIX)]

                if not document_links:
                    print(f"[INFO] No document links found on page {page_number}. Exiting.")
                    break

                for doc_link in tqdm(document_links, desc=f"[INFO] Processing links from page {page_number}"):
                    transformed_link = transform_link(doc_link)
                    try:
                        c.execute("INSERT INTO found_links (url, timestamp) VALUES (?, ?)",
                                  (transformed_link, datetime.now().isoformat()))
                    except sqlite3.IntegrityError:
                        pass
                conn.commit()
                c.execute('UPDATE last_page SET page_number = ? WHERE id = 1', (page_number,))
                conn.commit()

                page_number += 1
                print("[INFO] Waiting...")
                await asyncio.sleep(timeout)

            except PlaywrightTimeoutError:
                print(f"[WARNING] Timeout on page {page_number}. Exiting.")
                break
            except Exception as e:
                print(f"[ERROR] Problem loading the page: {e}")
                print("[INFO] Reopening new page...")
                try:
                    await page.close()
                    page = await context.new_page()
                except Exception as reopen_error:
                    print(f"[CRITICAL] Cannot open new page: {reopen_error}")
                    break

        await browser.close()

# === MAIN ===

def main():
    print(banner)

    parser = argparse.ArgumentParser(description="*RavenHunter* allows you to easily extract and retrieve RDP-CIA documents from the CIA database under the Freedom of Information Act.")
    parser.add_argument("-dl", action="store_true", help="Download found documents")
    parser.add_argument("--timeout", type=int, default=30, help="Timeout in seconds between each page scrape (default=30)")
    parser.add_argument("--resume", action="store_true", help="Resume scraping from the last page")
    parser.add_argument("--export", choices=["json", "csv"], help="Export found links as file")
    parser.add_argument("--verbose", action="store_true", help="Display generated links during search")
    parser.add_argument("--tor", action="store_true", help="Use Tor proxy (127.0.0.1:9050)")
    args = parser.parse_args()

    if args.tor and not tor_service_running():
        print("[ERROR] Tor is not running.")
        print("Please install and start the Tor service. On Windows, use the Expert Bundle. On Linux, ensure tor is installed and running.")
        sys.exit(1)

    conn, c = init_db()
    show_summary()

    if args.export or args.dl:
        if args.export:
            export_found_links(args.export)
        if args.dl:
            download_found_documents()
        return

    if not args.resume:
        last_page = get_last_page(conn)
        if last_page > 0:
            answer = input(f"[PROMPT] A previous session was detected (last scraped page: {last_page}). Do you want to continue from there? (y/n): ").strip().lower()
            if answer == 'y':
                resume_page = last_page
            else:
                resume_page = 0
        else:
            resume_page = 0
    else:
        resume_page = get_last_page(conn) or 1

    try:
        asyncio.run(scrape_documents(args.timeout, resume_page, args.verbose, conn, c, use_tor=args.tor))
    except KeyboardInterrupt:
        print("\n[INFO] Scraping interrupted. Saving progress...")

    show_summary()

if __name__ == "__main__":
    main()
