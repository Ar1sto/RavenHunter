import requests
import sqlite3
import asyncio
import argparse
import json
import csv
import os
import sys
from tqdm import tqdm
from datetime import datetime
from colorama import init as enable
from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError
from bs4 import BeautifulSoup

enable()
if os.name == "nt":
    os.system("chcp 65001 > nul")

DB_NAME = "cia_docs.db"

BASE_URL = "https://www.cia.gov/readingroom/search/site/*?page={}"
DOCUMENT_PREFIX = "https://www.cia.gov/readingroom/document/"

HEADERS = {
    "User-Agent": "Mozilla/5.0 (compatible; Project-RavenHunter/3.0)"
}

banner = """
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣀⣀⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣴⣿⣿⡟⠋⢻⣷⣄⡀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣤⣾⣿⣷⣿⣿⣿⣿⣿⣶⣾⣿⣿⠿⠿⠿⠶⠄⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⠉⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡟⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠃⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠃⠀⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⢸⣿⣿⣿⣿⣿⣿⣿⣿⡟⠁⠀⠀⠀⠀⠀
╔═══════════════════════════════════════════════════╗⠀⠀⠀⠀
║ ╦═╗┌─┐┬  ┬┌─┐┌┐┌╦ ╦┬ ┬┌┐┌┌┬┐┌─┐┬─┐ v3.1.0         ║
║ ╠╦╝├─┤└┐┌┘├┤ │││╠═╣│ ││││ │ ├┤ ├┬┘                ║
║ ╩╚═┴ ┴ └┘ └─┘┘└┘╩ ╩└─┘┘└┘ ┴ └─┘┴└─ ~ by 0mniscius ║
║ [+] CIA RDP-Document Scraping & Dumping Tool      ║
╚═══════════════════════════════════════════════════╝⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠸⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠉⠙⠛⠋⠉⠉⠀⠀⠀⠀⠀⠀
"""

# === DATABASE SETUP ===

def init_db():
    conn = sqlite3.connect(DB_NAME)
    c = conn.cursor()
    c.execute("""CREATE TABLE IF NOT EXISTS found_links (
                    url TEXT PRIMARY KEY,
                    timestamp TEXT
                )""")
    c.execute("""CREATE TABLE IF NOT EXISTS last_page (
                    id INTEGER PRIMARY KEY CHECK (id = 1),
                    page_number INTEGER DEFAULT 0
                )""")
    c.execute('INSERT OR IGNORE INTO last_page (id, page_number) VALUES (1, 0)')
    conn.commit()
    return conn, c


# === LOGIC ===

def export_found_links(format="json"):
    links = get_all_found_links()
    if not links:
        print("No links found to export.")
        return

    header = f"Generated by *RavenHunter*\n{banner}\n"

    if format == "json":
        with open("cia_docs_export.json", "w", encoding="utf-8") as f:
            f.write(header)
            json.dump(links, f, indent=4)
        print("[✔] Export as JSON completed: cia_docs_export.json")

    elif format == "csv":
        with open("cia_docs_export.csv", "w", newline='', encoding="utf-8") as f:
            writer = csv.writer(f)
            f.write(header)
            writer.writerow(["URL"])
            for url in links:
                writer.writerow([url])
        print("[✔] Export as CSV completed: cia_docs_export.csv")

    else:
        print(f"[!] Unknown export format: {format}")


def log_found(url):
    with sqlite3.connect(DB_NAME) as conn:
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        conn.execute("INSERT OR IGNORE INTO found_links (url, timestamp) VALUES (?, ?)", (url, timestamp))
        conn.commit()


def get_all_found_links():
    with sqlite3.connect(DB_NAME) as conn:
        c = conn.execute("SELECT url FROM found_links")
        return [row[0] for row in c.fetchall()]


def show_summary():
    with sqlite3.connect(DB_NAME) as conn:
        c = conn.execute("SELECT COUNT(*) FROM found_links")
        count = c.fetchone()[0]
        print(f"[✔] Documents already extracted: {count}")


def check_link(url):
    try:
        r = requests.head(url, headers=HEADERS, timeout=10, allow_redirects=True)
        return r.status_code == 200
    except requests.RequestException as e:
        print(f"[ERROR] Request failed for {url}: {e}")
        return False

# === SCRAPE FUNCTIONS ===

def transform_link(original_link):
    doc_id = original_link.split("/document/")[-1]
    doc_id_upper = doc_id.upper()
    return f"https://www.cia.gov/readingroom/docs/{doc_id_upper}.pdf"

async def scrape_documents(timeout, resume_page, verbose, conn, c):
    page_number = resume_page

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)  # Headless False, da Bug wenn im Hintergrund ausgeführt (DOM-Elemente werden nicht gerendert)
        context = await browser.new_context(extra_http_headers=HEADERS)

        while True:
            url = BASE_URL.format(page_number)
            print(f"\n[INFO] Loading page {page_number}...")

            page = await context.new_page()
            try:
                await page.goto(url, wait_until="networkidle", timeout=10000)

                print("[INFO] Waiting for at least one <h3 class='title'>-element...")
                await page.wait_for_selector("h3.title a", timeout=5000)

                print("[INFO] Retrieving HTML content...")
                content = await page.content()


                if page_number > get_last_page(conn):
                    c.execute('UPDATE last_page SET page_number = ? WHERE id = 1', (page_number,))
                    conn.commit()


            except PlaywrightTimeoutError:
                print(f"[WARNING] Timeout on page {page_number}. Exiting.")
                break
            except Exception as e:
                print(f"[ERROR] Problem loading the page: {e}")
                break
            finally:
                await page.close()

            soup = BeautifulSoup(content, "html.parser")
            h3_titles = soup.find_all('h3', class_='title')

            document_links = []
            for h3 in h3_titles:
                a_tag = h3.find('a', href=True)
                if a_tag and a_tag['href'].startswith(DOCUMENT_PREFIX):
                    document_links.append(a_tag['href'])

            if not document_links:
                print(f"[INFO] No document links found on page {page_number}. Exiting.")
                break

            for doc_link in tqdm(document_links, desc=f"Processing links from page {page_number}"):
                transformed_link = transform_link(doc_link)
                log_found(transformed_link)
                if verbose:
                    print(f"[INFO] Found link: {transformed_link}")

            page_number += 1
            print("[INFO] Waiting...")
            await asyncio.sleep(timeout)  

            with sqlite3.connect(DB_NAME) as conn:
                conn.execute("UPDATE last_page SET page_number = ? WHERE id = 1", (page_number,))
                conn.commit()

        await browser.close()


def download_found_documents():
    links = get_all_found_links()
    out_folder = "cia_archive"
    os.makedirs(out_folder, exist_ok=True)

    for url in tqdm(links, desc="[+] Download found documents..."):
        filename = os.path.join(out_folder, url.split("/")[-1])
        if os.path.exists(filename):
            continue
        try:
            r = requests.get(url, headers=HEADERS)#, timeout=0)
            if r.status_code == 200:
                with open(filename, "wb") as f:
                    f.write(r.content)
        except KeyboardInterrupt:
            print("[!] Download aborted.")
            sys.exit(3)
        except Exception as e:
            print(f"[!] Error by {url}: {e}")


# === GET HIGHEST PAGE ===

def get_last_page(conn):
    c = conn.cursor()
    c.execute('SELECT page_number FROM last_page WHERE id = 1')
    result = c.fetchone()
    return result[0] if result else 0


# === MAIN ===

def main():
    print(banner)

    parser = argparse.ArgumentParser(description="*RavenHunter* allows you to easily extract and retrieve RDP-CIA documents from the CIA database under the Freedom of Information Act.")
    parser.add_argument("-dl", action="store_true", help="Download found documents")
    parser.add_argument("--timeout", type=int, default=10, help="Timeout in seconds between each page scrape")
    parser.add_argument("--resume", action="store_true", help="Resume scraping from the last page")
    parser.add_argument("--export", choices=["json", "csv"], help="Export found links as file")
    parser.add_argument("--verbose", action="store_true", help="Display generated links during search")
    args = parser.parse_args()

    conn, c = init_db()
    show_summary()

# === ARGS ===

    if args.export or args.dl:
        if args.export:
            export_found_links(args.export)
        if args.dl:
            download_found_documents()
        return


    if not args.resume: 
        last_page = get_last_page(conn)
        if last_page > 0:
            answer = input(f"[PROMPT] A previous session was detected (last scraped page: {last_page}). Do you want to continue from there? (y/n): ").strip().lower()
            if answer == 'y':
                resume_page = last_page
            else:
                resume_page = 0
        else:
            resume_page = 0  
    else:
        resume_page = get_last_page(conn) or 1  

    try:
        asyncio.run(scrape_documents(args.timeout, resume_page, args.verbose, conn, c))

    except KeyboardInterrupt:
        print("\n[INFO] Scraping interrupted. Saving progress...")

    show_summary()


if __name__ == "__main__":
    main()
